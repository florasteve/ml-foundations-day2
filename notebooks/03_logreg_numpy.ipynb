{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3093c2eb",
   "metadata": {},
   "source": [
    "# Day 2 — Logistic Regression (NumPy from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "np.random.seed(42)\n",
    "# Data\n",
    "X,y = make_classification(n_samples=600, n_features=2, n_redundant=0, n_informative=2,\n",
    "                          n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# Helpers\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def predict_proba(W,b,X):\n",
    "    return sigmoid(X@W + b)\n",
    "def predict(W,b,X):\n",
    "    return (predict_proba(W,b,X)>=0.5).astype(int)\n",
    "def bce_loss(y_hat,y):\n",
    "    eps=1e-9\n",
    "    return -(y*np.log(y_hat+eps)+(1-y)*np.log(1-y_hat+eps)).mean()\n",
    "# Train (batch GD)\n",
    "def fit(X,y,lr=0.1,epochs=2000):\n",
    "    n,d = X.shape\n",
    "    W = np.zeros(d); b = 0.0\n",
    "    for t in range(epochs):\n",
    "        y_hat = predict_proba(W,b,X)\n",
    "        grad_W = X.T@(y_hat - y)/n\n",
    "        grad_b = (y_hat - y).mean()\n",
    "        W -= lr*grad_W; b -= lr*grad_b\n",
    "        if t%400==0:\n",
    "            print(f'epoch {t}: loss={bce_loss(y_hat,y):.4f}')\n",
    "    return W,b\n",
    "W,b = fit(X_train,y_train,lr=0.1,epochs=2000)\n",
    "train_acc = (predict(W,b,X_train)==y_train).mean()\n",
    "test_acc  = (predict(W,b,X_test)==y_test).mean()\n",
    "print('NumPy train acc:', round(train_acc,3), ' test acc:', round(test_acc,3))\n",
    "# Sklearn parity\n",
    "sk = SkLogReg().fit(X_train,y_train)\n",
    "print('sklearn train acc:', round(sk.score(X_train,y_train),3), ' test acc:', round(sk.score(X_test,y_test),3))\n",
    "# Decision boundary\n",
    "xx,yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n",
    "                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "zz = predict(W,b,grid).reshape(xx.shape)\n",
    "plt.contourf(xx,yy,zz,alpha=0.2)\n",
    "plt.scatter(X_test[:,0],X_test[:,1],c=y_test,edgecolor='k',alpha=0.8)\n",
    "plt.title('NumPy logistic regression — decision boundary'); plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
